{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86c0f266-cabc-49dd-a58a-adeb3878c55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ainize/bart-base-cnn were not used when initializing BartForSequenceClassification: ['lm_head.weight', 'final_logits_bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at ainize/bart-base-cnn and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Download and load pre-trained GloVe embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', no_header=True)\n",
    "\n",
    "# Download and load pre-trained ELMo model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ainize/bart-base-cnn\")\n",
    "elmo_model = AutoModelForSequenceClassification.from_pretrained(\"ainize/bart-base-cnn\")\n",
    "\n",
    "# Define a function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "  # Lowercase, remove punctuation, tokenize\n",
    "  text = text.lower().strip().replace(\",\", \" \").replace(\".\", \" \")\n",
    "  tokens = tokenizer.tokenize(text, return_tensors=\"pt\")\n",
    "  return tokens\n",
    "\n",
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"imdb\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data[\"train\"][\"text\"], data[\"train\"][\"label\"], test_size=0.2\n",
    ")\n",
    "\n",
    "# Function to get word embeddings (Glove or ELMO)\n",
    "def get_embeddings(text, model_type=\"glove\"):\n",
    "  embeddings = []\n",
    "  for word in text.split():\n",
    "    if model_type == \"glove\":\n",
    "      # Check if word exists in vocabulary\n",
    "      if word in glove_model.key_to_index:\n",
    "        embeddings.append(glove_model[word])\n",
    "      else:\n",
    "        embeddings.append(np.zeros(100))  # Use zero vector for missing words\n",
    "    elif model_type == \"elmo\":\n",
    "      # Get ELMo embeddings for the entire sentence\n",
    "      with torch.no_grad():\n",
    "        inputs = preprocess_text(text)\n",
    "        encoded_layers = elmo_model(*inputs)\n",
    "        embeddings = encoded_layers[0].squeeze(0).cpu().numpy()\n",
    "    else:\n",
    "      raise ValueError(\"Invalid model type. Choose 'glove' or 'elmo'.\")\n",
    "  # Average word embeddings to get sentence embedding\n",
    "  return np.mean(embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5dc80f6-62a8-4e1e-90cb-71863fea390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models with Glove and ELMO embeddings\n",
    "def train_evaluate_model(embeddings, train_labels, test_labels):\n",
    "  # Train a logistic regression model\n",
    "  model = LogisticRegression(max_iter=1000)\n",
    "  model.fit(embeddings, train_labels)\n",
    "  predictions = model.predict(embeddings)\n",
    "  accuracy = np.mean(predictions == train_labels)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0eb1ea30-184e-4ae7-9890-28c704173317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove embedding accuracy: 0.7876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate Glove and ELMO embeddings\n",
    "glove_embeddings = [get_embeddings(text, model_type=\"glove\") for text in train_texts]\n",
    "#elmo_embeddings = [get_embeddings(text, model_type=\"elmo\") for text in train_texts]\n",
    "\n",
    "glove_accuracy = train_evaluate_model(glove_embeddings, train_labels, test_labels)\n",
    "#elmo_accuracy = train_evaluate_model(elmo_embeddings, train_labels, test_labels)\n",
    "\n",
    "print(\"Glove embedding accuracy:\", glove_accuracy)\n",
    "#print(\"ELMo embedding accuracy:\", elmo_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415201d-7171-466a-bbdd-3cbb6e973717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
